{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Models on Subsets Dataset\n",
    " - We suspect that our modified dataset will yield better results than the 'raw' normalized dataset. In this notebook we will investigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required imports and dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import preprocessing\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, Flatten\n",
    "from keras.metrics import *\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "#our own confusion matrix module that we developed\n",
    "import cmatrix as cm\n",
    "\n",
    "#load data into data frames\n",
    "xlsPath = r'C:\\Users\\Daniel Patrick\\Documents\\uniwork\\ada\\ada2\\Subsets.xlsx'\n",
    "data = pd.read_excel(xlsPath)\n",
    "\n",
    "#split into data and class variable\n",
    "X = data.drop(['date','price increase tomorrow?'], axis =1)\n",
    "class_var = data['price increase tomorrow?']\n",
    "\n",
    "\n",
    "#perform normalization on data \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "for column in X:\n",
    "    X[column] = min_max_scaler.fit_transform(X[column].values.reshape(-1,1))\n",
    "\n",
    "import pywt\n",
    "import statistics\n",
    "for column in X:\n",
    "    coeff = pywt.wavedec(X[column], \"haar\", level=10)\n",
    "    sigma = statistics.median(coeff[-1])/0.6745\n",
    "    threshold = sigma*np.sqrt(2*np.log(len(X[column])))\n",
    "    coeff[1:] = (pywt.threshold(i, value=threshold) for i in coeff[1:])\n",
    "    X[column] = pywt.waverec(coeff, \"haar\")\n",
    "\n",
    "#select dimensions to reduce to \n",
    "inputDims = 3\n",
    "attributes = SelectKBest(chi2, k=inputDims).fit_transform(X,class_var)\n",
    "\n",
    "\n",
    "#Create a train test split\n",
    "split_number = round(len(data)*0.97)\n",
    "train_attributes,train_class_var,test_attributes,test_class_var = \\\n",
    "attributes[:split_number],class_var[:split_number], \\\n",
    "attributes[split_number:],class_var[split_number:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequentialNN(train_attributes,train_class_var,test_attributes,test_class_var):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # The imput dim here is the number of cols in the df getting fed into the model\n",
    "    model.add(Dense(64, input_dim=inputDims, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(train_attributes, train_class_var,\n",
    "             epochs=250, verbose=0, batch_size=128)\n",
    "\n",
    "    #the predict_classes method returns the binary variable we're looking for\n",
    "    # for some reason it doesn't output the correct array type, made a 2D array\n",
    "    pred = model.predict_classes(test_attributes)\n",
    "\n",
    "    #the flatten method solves this and squashes to a 1D array for evaluation\n",
    "    predictions = pred.flatten()\n",
    "\n",
    "    #be sure to import our cmatrix module\n",
    "    cmat = cm.cmatrix(test_class_var, predictions)\n",
    "    \n",
    "    model.reset_states()\n",
    "    return cmat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logRegression(train_attributes,train_class_var,test_attributes,\n",
    "                  test_class_var):\n",
    "    \n",
    "    #Below is the list of hyper-parameters which produced\n",
    "    #the best results\n",
    "    logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                C=1.0, fit_intercept=True, solver='liblinear')\n",
    "    logreg.fit(train_attributes, train_class_var)\n",
    "    predictions = logreg.predict(test_attributes)\n",
    "    \n",
    "    cmat = cm.cmatrix(test_class_var, predictions)\n",
    "    return cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(train_attributes,train_class_var,test_attributes,test_class_var):\n",
    "    \n",
    "    random_forest = RandomForestClassifier(n_estimators=20)\n",
    "    random_forest.fit(train_attributes, train_class_var)\n",
    "    predictions = random_forest.predict(test_attributes)\n",
    "    \n",
    "    cmat = cm.cmatrix(test_class_var, predictions)\n",
    "    return cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(train_attributes,train_class_var,test_attributes,test_class_var):\n",
    "    \n",
    "    svc = SVC()\n",
    "    svc.fit(train_attributes, train_class_var)\n",
    "    predictions = svc.predict(test_attributes)\n",
    "    \n",
    "    cmat = cm.cmatrix(test_class_var, predictions)\n",
    "    return cmat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(train_attributes,train_class_var,test_attributes,test_class_var):\n",
    "    \n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(train_attributes, train_class_var)\n",
    "    predictions = perceptron.predict(test_attributes)\n",
    "    \n",
    "    cmat = cm.cmatrix(test_class_var, predictions)\n",
    "    return cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequentialNN(train_attributes,train_class_var,test_attributes,test_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegression(train_attributes,train_class_var,test_attributes,test_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron(train_attributes,train_class_var,test_attributes,test_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest(train_attributes,train_class_var,test_attributes,test_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc(train_attributes,train_class_var,test_attributes,test_class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
